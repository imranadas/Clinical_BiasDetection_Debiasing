{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f989af7-da20-448a-992b-ddece1c91b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import networkx as nx\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d7870-ad7b-41f7-9e69-1f26736ac5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b890c-6eb4-4802-917b-102e990abda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f14ed9-bc99-4749-8196-ed471e92f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract POS tag features\n",
    "def extract_pos_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    pos_counts = {\n",
    "        \"NOUN\": 0,\n",
    "        \"VERB\": 0,\n",
    "        \"ADJ\": 0,\n",
    "        \"ADV\": 0,\n",
    "        \"OTHER\": 0\n",
    "    }\n",
    "    for token in doc:\n",
    "        if token.pos_ in pos_counts:\n",
    "            pos_counts[token.pos_] += 1\n",
    "        else:\n",
    "            pos_counts[\"OTHER\"] += 1\n",
    "    return [pos_counts[\"NOUN\"], pos_counts[\"VERB\"], pos_counts[\"ADJ\"], pos_counts[\"ADV\"], pos_counts[\"OTHER\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a74613-b30a-42b8-953e-d5054986801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract dependency features\n",
    "def extract_dependency_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dep_counts = {\n",
    "        \"nsubj\": 0,\n",
    "        \"dobj\": 0,\n",
    "        \"ROOT\": 0,\n",
    "        \"amod\": 0,\n",
    "        \"OTHER\": 0\n",
    "    }\n",
    "    for token in doc:\n",
    "        if token.dep_ in dep_counts:\n",
    "            dep_counts[token.dep_] += 1\n",
    "        else:\n",
    "            dep_counts[\"OTHER\"] += 1\n",
    "    return [dep_counts[\"nsubj\"], dep_counts[\"dobj\"], dep_counts[\"ROOT\"], dep_counts[\"amod\"], dep_counts[\"OTHER\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c38b1-fd28-43bf-8e74-28a10bfe8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract sentiment features\n",
    "def extract_sentiment_features(sentence):\n",
    "    sentiment = analyzer.polarity_scores(sentence)\n",
    "    return [sentiment[\"neg\"], sentiment[\"neu\"], sentiment[\"pos\"], sentiment[\"compound\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d5541-db5f-4088-be7f-1e7ba74a8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_graphical_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    G = nx.DiGraph()\n",
    "    for token in doc:\n",
    "        G.add_node(token.text, pos=token.pos_, dep=token.dep_)\n",
    "        if token.head != token:\n",
    "            G.add_edge(token.head.text, token.text, label=token.dep_)\n",
    "    \n",
    "    # Extract features\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    avg_node_degree = np.mean([deg for _, deg in G.degree()]) if num_nodes > 0 else 0\n",
    "    density = nx.density(G) if num_nodes > 1 else 0\n",
    "    root_token = [token for token in doc if token.dep_ == \"ROOT\"][0].text if num_nodes > 0 else \"\"\n",
    "    \n",
    "    return [num_nodes, num_edges, avg_node_degree, density, len(root_token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5efcd-7022-4c63-b022-cbd6b9a349ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract word embeddings using SpaCy\n",
    "def extract_word_embeddings(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return np.mean([token.vector for token in doc], axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3850c-f02d-46b4-976a-2a62afcaa827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract contextual embeddings using BERT\n",
    "def extract_contextual_embeddings(sentence):\n",
    "    inputs = bert_tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    # Use the mean of the last hidden state to represent the sentence\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd196a-96f4-4131-a8c8-c7b440588485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to generate feature vector for each sentence\n",
    "# def generate_feature_vector(sentence):\n",
    "#     pos_features = extract_pos_features(sentence)\n",
    "#     dep_features = extract_dependency_features(sentence)\n",
    "#     sentiment_features = extract_sentiment_features(sentence)\n",
    "#     graphical_features = extract_graphical_features(sentence)\n",
    "#     word_embeddings = extract_word_embeddings(sentence)\n",
    "#     contextual_embeddings = extract_contextual_embeddings(sentence)\n",
    "#     return pos_features + dep_features + sentiment_features + graphical_features + word_embeddings + contextual_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3e8a0-b97d-4b00-878b-523dd572e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate feature vector for each sentence\n",
    "def generate_feature_vector(sentence):\n",
    "    pos_features = extract_pos_features(sentence)\n",
    "    dep_features = extract_dependency_features(sentence)\n",
    "    sentiment_features = extract_sentiment_features(sentence)\n",
    "    graphical_features = extract_graphical_features(sentence)\n",
    "    word_embeddings = extract_word_embeddings(sentence)\n",
    "    # contextual_embeddings = extract_contextual_embeddings(sentence)\n",
    "    return pos_features + dep_features + sentiment_features + graphical_features + word_embeddings # + contextual_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158924f8-35dd-4037-9df1-7ebff93c845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset for training the classifier\n",
    "sentences = [\n",
    "    \"The black patient was prescribed medication.\",\n",
    "    \"The white doctor treated the patient.\",\n",
    "    \"The nurse assisted the patient during recovery.\",\n",
    "    \"The hispanic family was very cooperative.\",\n",
    "    \"The caucasian man was given special treatment.\",\n",
    "    \"The patient was treated with care.\",\n",
    "    \"The doctor was very professional.\",\n",
    "    \"The nurse provided excellent support.\",\n",
    "    \"The family was supportive throughout the treatment.\",\n",
    "    \"The man was given the necessary treatment.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf71ff-9cf5-4dda-90e3-cfd12d0566f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for the dataset (1 for biased, 0 for non-biased)\n",
    "labels = [1, 1, 0, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e150e-21d4-46b9-8684-41f5b5cbb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature vectors for all sentences\n",
    "feature_vectors = np.array([generate_feature_vector(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f1f84-a475-4d4f-ae65-2827c8e0e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fab171-6aa9-425a-b3cd-96b377c9c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the SVM classifier\n",
    "svm_classifier = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af6322-2696-440b-912c-528ef1a294da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01286e-2a30-414f-b761-3ad7f3aedddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify if a given sentence is biased and detect biased phrases\n",
    "def classify_bias(sentence):\n",
    "    X_new = np.array([generate_feature_vector(sentence)])  # Generate feature vector for the new sentence\n",
    "    is_biased = svm_classifier.predict(X_new)[0]  # Predict if the sentence is biased\n",
    "    \n",
    "    biased_phrases = []\n",
    "    if is_biased:\n",
    "        # Use the linguistic features to detect biased phrases\n",
    "        biased_phrases = []  # No longer identifying specific biased phrases based on predefined racial terms\n",
    "    \n",
    "    return is_biased, biased_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83152609-0494-4c2b-a820-5856403ca8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the bias classification function\n",
    "test_sentences = [\n",
    "    \"The black patient was prescribed medication.\",\n",
    "    \"The white doctor treated the patient.\",\n",
    "    \"The nurse assisted the patient during recovery.\",\n",
    "    \"The hispanic family was very cooperative.\",\n",
    "    \"The caucasian man was given special treatment.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94675d-c5f8-40c3-a1ba-4268f12b0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify bias in each sentence\n",
    "for sentence in test_sentences:\n",
    "    is_biased, biased_phrases = classify_bias(sentence)\n",
    "    if is_biased:\n",
    "        print(f\"Biased Sentence: '{sentence}'\")\n",
    "        print(f\"Biased Phrases Detected: {[(phrase[0], phrase[1], phrase[2]) for phrase in biased_phrases]}\")\n",
    "    else:\n",
    "        print(f\"Non-biased Sentence: '{sentence}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf60ff4-8f6c-4b3f-ad48-c046927a4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze training data behavior and plot dependency patterns\n",
    "def plot_dependency_patterns(sentences, labels):\n",
    "    dependency_patterns = []\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        if label == 1:  # Only consider biased sentences\n",
    "            doc = nlp(sentence)\n",
    "            for token in doc:\n",
    "                dependency_patterns.append(token.dep_)\n",
    "    \n",
    "    # Count dependency patterns\n",
    "    dep_counts = Counter(dependency_patterns)\n",
    "    \n",
    "    # Plot the dependency patterns\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(dep_counts.keys()), y=list(dep_counts.values()))\n",
    "    plt.title(\"Most Observed Dependency Patterns in Biased Sentences\")\n",
    "    plt.xlabel(\"Dependency Type\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618673b-946a-4a05-bab9-b0208979b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the most contributing linguistic features\n",
    "def plot_linguistic_features_importance(svm_classifier, feature_names):\n",
    "    if hasattr(svm_classifier, 'coef_'):\n",
    "        coefs = svm_classifier.named_steps['svc'].coef_.flatten()\n",
    "        feature_importance = sorted(zip(coefs, feature_names), key=lambda x: abs(x[0]), reverse=True)\n",
    "        top_features = feature_importance[:10]\n",
    "        \n",
    "        # Plot the top contributing features\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=[x[1] for x in top_features], y=[x[0] for x in top_features])\n",
    "        plt.title(\"Top Contributing Linguistic Features\")\n",
    "        plt.xlabel(\"Feature\")\n",
    "        plt.ylabel(\"Coefficient Value\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85da2a-3c12-4e32-8205-0be699fd1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature names for plotting importance\n",
    "feature_names = [\n",
    "    \"NOUN_count\", \"VERB_count\", \"ADJ_count\", \"ADV_count\", \"OTHER_POS_count\",\n",
    "    \"nsubj_count\", \"dobj_count\", \"ROOT_count\", \"amod_count\", \"OTHER_DEP_count\",\n",
    "    \"negative_sentiment\", \"neutral_sentiment\", \"positive_sentiment\", \"compound_sentiment\",\n",
    "    \"num_nodes\", \"num_edges\", \"avg_node_degree\", \"density\", \"root_token_length\"\n",
    "] + [f\"word_embedding_{i}\" for i in range(96)] + [f\"contextual_embedding_{i}\" for i in range(768)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6233867-7c34-4819-8324-f912ca8caf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dependency patterns and linguistic feature importance\n",
    "plot_dependency_patterns(sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4856a7-16d3-4f60-b05c-d72940a1a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linguistic_features_importance(svm_classifier, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31809612-625c-42cc-9b4d-265da52cef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAT Implementation to analyze bias in word embeddings\n",
    "def weat_score(target_words_1, target_words_2, attribute_words_1, attribute_words_2):\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def association(word, attribute_words):\n",
    "        return np.mean([cosine_similarity(word, attr) for attr in attribute_words])\n",
    "\n",
    "    # Convert words to embeddings using SpaCy\n",
    "    target_vecs_1 = [nlp(word).vector for word in target_words_1]\n",
    "    target_vecs_2 = [nlp(word).vector for word in target_words_2]\n",
    "    attribute_vecs_1 = [nlp(word).vector for word in attribute_words_1]\n",
    "    attribute_vecs_2 = [nlp(word).vector for word in attribute_words_2]\n",
    "\n",
    "    # Calculate WEAT score\n",
    "    mean_diff_1 = np.mean([association(target, attribute_vecs_1) - association(target, attribute_vecs_2) for target in target_vecs_1])\n",
    "    mean_diff_2 = np.mean([association(target, attribute_vecs_1) - association(target, attribute_vecs_2) for target in target_vecs_2])\n",
    "    return mean_diff_1 - mean_diff_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a032e5-1955-429a-abe2-e45e0a5eda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example words for WEAT\n",
    "# Target words representing different racial groups\n",
    "target_words_1 = [\"black\", \"african\", \"hispanic\"]\n",
    "target_words_2 = [\"white\", \"caucasian\", \"european\"]\n",
    "# Attribute words representing healthcare-related associations\n",
    "attribute_words_1 = [\"caring\", \"supportive\", \"professional\", \"competent\"]\n",
    "attribute_words_2 = [\"neglectful\", \"uncooperative\", \"incompetent\", \"rude\"]\n",
    "\n",
    "# Calculate and print WEAT score\n",
    "weat_result = weat_score(target_words_1, target_words_2, attribute_words_1, attribute_words_2)\n",
    "print(f\"WEAT Score: {weat_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7800a-5daa-4e75-8751-44dbd974f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import networkx as nx\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the SpaCy model (English model in this case)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")  # Attempt to load the pre-trained SpaCy model\n",
    "except OSError:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"en_core_web_sm\")  # Download the model if not found\n",
    "    nlp = spacy.load(\"en_core_web_sm\")  # Load the model after downloading\n",
    "\n",
    "# Load BERT tokenizer and model for contextual embeddings\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# List of racial terms and phrases to check for potential bias\n",
    "# racial_terms = [\n",
    "#     \"black\", \"white\", \"hispanic\", \"asian\", \"indian\", \"native\", \"african\",\n",
    "#     \"caucasian\", \"latino\", \"arab\", \"middle eastern\"\n",
    "# ]\n",
    "\n",
    "# Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to extract POS tag features\n",
    "def extract_pos_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    pos_counts = {\n",
    "        \"NOUN\": 0,\n",
    "        \"VERB\": 0,\n",
    "        \"ADJ\": 0,\n",
    "        \"ADV\": 0,\n",
    "        \"OTHER\": 0\n",
    "    }\n",
    "    for token in doc:\n",
    "        if token.pos_ in pos_counts:\n",
    "            pos_counts[token.pos_] += 1\n",
    "        else:\n",
    "            pos_counts[\"OTHER\"] += 1\n",
    "    return [pos_counts[\"NOUN\"], pos_counts[\"VERB\"], pos_counts[\"ADJ\"], pos_counts[\"ADV\"], pos_counts[\"OTHER\"]]\n",
    "\n",
    "# Function to extract dependency features\n",
    "def extract_dependency_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dep_counts = {\n",
    "        \"nsubj\": 0,\n",
    "        \"dobj\": 0,\n",
    "        \"ROOT\": 0,\n",
    "        \"amod\": 0,\n",
    "        \"OTHER\": 0\n",
    "    }\n",
    "    for token in doc:\n",
    "        if token.dep_ in dep_counts:\n",
    "            dep_counts[token.dep_] += 1\n",
    "        else:\n",
    "            dep_counts[\"OTHER\"] += 1\n",
    "    return [dep_counts[\"nsubj\"], dep_counts[\"dobj\"], dep_counts[\"ROOT\"], dep_counts[\"amod\"], dep_counts[\"OTHER\"]]\n",
    "\n",
    "# Function to extract sentiment features\n",
    "def extract_sentiment_features(sentence):\n",
    "    sentiment = analyzer.polarity_scores(sentence)\n",
    "    return [sentiment[\"neg\"], sentiment[\"neu\"], sentiment[\"pos\"], sentiment[\"compound\"]]\n",
    "\n",
    "# Function to extract graphical features from the dependency graph\n",
    "def extract_graphical_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    G = nx.DiGraph()\n",
    "    for token in doc:\n",
    "        G.add_node(token.text, pos=token.pos_, dep=token.dep_)\n",
    "        if token.head != token:\n",
    "            G.add_edge(token.head.text, token.text, label=token.dep_)\n",
    "    \n",
    "    # Extract features\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    avg_node_degree = np.mean([deg for _, deg in G.degree()]) if num_nodes > 0 else 0\n",
    "    density = nx.density(G) if num_nodes > 1 else 0\n",
    "    root_token = [token for token in doc if token.dep_ == \"ROOT\"][0].text if num_nodes > 0 else \"\"\n",
    "    \n",
    "    return [num_nodes, num_edges, avg_node_degree, density, len(root_token)]\n",
    "\n",
    "# Function to extract word embeddings using SpaCy\n",
    "def extract_word_embeddings(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return np.mean([token.vector for token in doc], axis=0).tolist()\n",
    "\n",
    "# Function to extract contextual embeddings using BERT\n",
    "def extract_contextual_embeddings(sentence):\n",
    "    inputs = bert_tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    # Use the mean of the last hidden state to represent the sentence\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings.tolist()\n",
    "\n",
    "# Function to generate feature vector for each sentence\n",
    "def generate_feature_vector(sentence):\n",
    "    pos_features = extract_pos_features(sentence)\n",
    "    dep_features = extract_dependency_features(sentence)\n",
    "    sentiment_features = extract_sentiment_features(sentence)\n",
    "    graphical_features = extract_graphical_features(sentence)\n",
    "    word_embeddings = extract_word_embeddings(sentence)\n",
    "    contextual_embeddings = extract_contextual_embeddings(sentence)\n",
    "    return pos_features + dep_features + sentiment_features + graphical_features + word_embeddings + contextual_embeddings\n",
    "\n",
    "# Sample dataset for training the classifier\n",
    "sentences = [\n",
    "    \"The black patient was prescribed medication.\",\n",
    "    \"The white doctor treated the patient.\",\n",
    "    \"The nurse assisted the patient during recovery.\",\n",
    "    \"The hispanic family was very cooperative.\",\n",
    "    \"The caucasian man was given special treatment.\",\n",
    "    \"The patient was treated with care.\",\n",
    "    \"The doctor was very professional.\",\n",
    "    \"The nurse provided excellent support.\",\n",
    "    \"The family was supportive throughout the treatment.\",\n",
    "    \"The man was given the necessary treatment.\"\n",
    "]\n",
    "\n",
    "# Labels for the dataset (1 for biased, 0 for non-biased)\n",
    "labels = [1, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Generate feature vectors for all sentences\n",
    "feature_vectors = np.array([generate_feature_vector(sentence) for sentence in sentences])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the SVM classifier\n",
    "svm_classifier = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Function to classify if a given sentence is biased and detect biased phrases\n",
    "def classify_bias(sentence):\n",
    "    X_new = np.array([generate_feature_vector(sentence)])  # Generate feature vector for the new sentence\n",
    "    is_biased = svm_classifier.predict(X_new)[0]  # Predict if the sentence is biased\n",
    "    \n",
    "    biased_phrases = []\n",
    "    if is_biased:\n",
    "        # Use the linguistic features to detect biased phrases\n",
    "        biased_phrases = identify_biased_phrases(sentence)\n",
    "    \n",
    "    return is_biased, biased_phrases\n",
    "\n",
    "# Function to identify biased phrases in a given sentence\n",
    "def identify_biased_phrases(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    biased_phrases = []\n",
    "    for token in doc:\n",
    "        if token.text.lower() in racial_terms:\n",
    "            biased_phrases.append((token.text, token.pos_, token.dep_))\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any(term in chunk.text.lower() for term in racial_terms):\n",
    "            biased_phrases.append((chunk.text, chunk.root.pos_, chunk.root.dep_))\n",
    "    biased_phrases = list({phrase[0]: phrase for phrase in biased_phrases}.values())\n",
    "    return biased_phrases\n",
    "\n",
    "# Test the bias classification function\n",
    "test_sentences = [\n",
    "    \"The black patient was prescribed medication.\",\n",
    "    \"The white doctor treated the patient.\",\n",
    "    \"The nurse assisted the patient during recovery.\",\n",
    "    \"The hispanic family was very cooperative.\",\n",
    "    \"The caucasian man was given special treatment.\"\n",
    "]\n",
    "\n",
    "# Classify bias in each sentence\n",
    "for sentence in test_sentences:\n",
    "    is_biased, biased_phrases = classify_bias(sentence)\n",
    "    if is_biased:\n",
    "        print(f\"Biased Sentence: '{sentence}'\")\n",
    "        print(f\"Biased Phrases Detected: {[(phrase[0], phrase[1], phrase[2]) for phrase in biased_phrases]}\")\n",
    "    else:\n",
    "        print(f\"Non-biased Sentence: '{sentence}'\")\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
