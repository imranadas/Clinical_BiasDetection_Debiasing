{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02257b-6d63-4709-af7a-bfdcd1472b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad5376-a14f-42a1-9c6d-a45c142c0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2') #get the GPT 2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a714dc-b1ea-4867-9ed7-d3d597a9a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (you can use your dataset instead)\n",
    "dataset = load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913639cf-df26-4ec2-9fae-cdc39d1dcba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset with labels for bias (1 for bias, 0 for non-bias)\n",
    "train_texts = [example['text'] for example in dataset['train']]\n",
    "train_labels = [1 if 'bias_keyword' in text.lower() else 0 for text in train_texts]  # Example labels, replace with real conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0e42e-b61d-4e8c-9aa3-936369d8535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data and add a classification label\n",
    "# Adding [BIAS] or [NONBIAS] label to the end of each text for classification purposes\n",
    "def tokenize_function(examples):\n",
    "    texts = []\n",
    "    for text, label in zip(examples['text'], examples['label']):\n",
    "        if label == 1:\n",
    "            texts.append(f\"{text} [BIAS]\")\n",
    "        else:\n",
    "            texts.append(f\"{text} [NONBIAS]\")\n",
    "    # Tokenizing texts with padding and truncation for uniform input size\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051f09c-a3e4-4e81-8943-b5a7d88b7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenized dataset\n",
    "tokenized_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928cf10-6864-47bc-84ee-8d75807c08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7902594-63bb-4044-961e-eb4fb49c95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1c259-3f51-4c27-9698-21a317001bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "# Setting up training parameters such as batch size, number of epochs, logging frequency, etc.\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# # Create Trainer instance\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset,\n",
    "# )\n",
    "\n",
    "# Create Trainer instance\n",
    "# Initializing the Trainer class with the model, training arguments, dataset, and tokenizer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "                                'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "                                'labels': torch.stack([f['label'] for f in data]).view(-1)}\n",
    "    # data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "    #                             'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "    #                             'labels': torch.stack([f['label'].unsqueeze(0) for f in data])}\n",
    "\n",
    "    # data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "    #                         'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "    #                         'labels': torch.tensor([f['label'] for f in data])}\n",
    "    \n",
    "    # data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "    #                         'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "    #                         'labels': torch.stack([torch.tensor(f['label']) for f in data])}\n",
    "\n",
    "    # data_collator=lambda data: {'input_ids': torch.nn.utils.rnn.pad_sequence([f['input_ids'] for f in data], batch_first=True),\n",
    "    #                             'attention_mask': torch.nn.utils.rnn.pad_sequence([f['attention_mask'] for f in data], batch_first=True),\n",
    "    #                             'labels': torch.tensor([f['label'] for f in data], dtype=torch.long)}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee30bd-0688-4970-8f9c-957cc8d4b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c104ef-1656-4251-b227-14b2642bd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fine-tuned model to classify new texts\n",
    "def classify_bias(text):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    output = model.generate(inputs, max_length=len(inputs[0]) + 10)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"[BIAS]\" in decoded_output:\n",
    "        return \"Bias detected\"\n",
    "    elif \"[NONBIAS]\" in decoded_output:\n",
    "        return \"Non-bias detected\"\n",
    "    else:\n",
    "        return \"Unclear\"\n",
    "\n",
    "# Test classification\n",
    "new_text = \"Some potentially biased text.\"\n",
    "print(classify_bias(new_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e1129-aef4-4465-8375-5c101c333a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "# BERT tokenizer is loaded to tokenize input text, and model is loaded to use pre-trained BERT for sequence classification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 labels: biased, non-biased\n",
    "\n",
    "# Load dataset (you can use your dataset instead)\n",
    "# Loading a pre-existing dataset called 'emotion' from Hugging Face\n",
    "# Replace 'emotion' with your dataset for bias detection\n",
    "dataset = load_dataset('emotion')\n",
    "\n",
    "# Prepare the dataset with labels for bias (1 for bias, 0 for non-bias)\n",
    "# Extracting texts and assigning labels manually based on a condition (e.g., keyword indicating bias)\n",
    "train_texts = [example['text'] for example in dataset['train']]\n",
    "train_labels = [1 if 'bias_keyword' in text.lower() else 0 for text in train_texts]  # Example labels, replace with real conditions\n",
    "\n",
    "# Tokenize the data and add labels\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Create tokenized dataset\n",
    "# Converting data to a Hugging Face Dataset format and setting format for PyTorch compatibility\n",
    "tokenized_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Split the dataset into training and evaluation\n",
    "train_dataset = tokenized_dataset.select(range(int(0.8 * len(tokenized_dataset))))\n",
    "eval_dataset = tokenized_dataset.select(range(int(0.8 * len(tokenized_dataset)), len(tokenized_dataset)))\n",
    "\n",
    "# Training arguments\n",
    "# Setting up training parameters such as batch size, number of epochs, logging frequency, etc.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save the training results\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    per_device_train_batch_size=8,  # Batch size for training\n",
    "    num_train_epochs=3,  # Number of epochs to train the model\n",
    "    save_steps=500,  # Save the model every 500 steps\n",
    "    logging_dir=\"./logs\",  # Directory to save the logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "# Initializing the Trainer class with the model, training arguments, dataset, and tokenizer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=lambda data: {\n",
    "        'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "        'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "        'labels': torch.tensor([f['label'] for f in data], dtype=torch.long)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# Start training the model using the Trainer instance\n",
    "trainer.train()\n",
    "\n",
    "# Use the fine-tuned model to classify new texts\n",
    "# Function to classify a given text as biased or non-biased using the fine-tuned model\n",
    "def classify_bias(text):\n",
    "    # Encode the input text to get input IDs for the model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits).item()\n",
    "    \n",
    "    # Check the predicted class\n",
    "    if predicted_class == 1:\n",
    "        return \"Bias detected\"\n",
    "    else:\n",
    "        return \"Non-bias detected\"\n",
    "\n",
    "# Test classification\n",
    "# Testing the classification function with a sample text\n",
    "new_text = \"Some potentially biased text.\"\n",
    "print(classify_bias(new_text))\n",
    "\n",
    "# Note: To run this script, please install the required dependencies by running the following command:\n",
    "# pip install transformers[torch] accelerate>=0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb060e6-29e9-43a4-90b7-03bf859b13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "\n",
    "# Set environment variables for M1 GPU compatibility\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "# BERT tokenizer is loaded to tokenize input text, and model is loaded to use pre-trained BERT for sequence classification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 labels: biased, non-biased\n",
    "\n",
    "# Load dataset (you can use your dataset instead)\n",
    "# Loading a pre-existing dataset called 'emotion' from Hugging Face\n",
    "# Replace 'emotion' with your dataset for bias detection\n",
    "dataset = load_dataset('emotion')\n",
    "\n",
    "# Prepare the dataset with labels for bias (1 for bias, 0 for non-bias)\n",
    "# Extracting texts and assigning labels manually based on a condition (e.g., keyword indicating bias)\n",
    "train_texts = [example['text'] for example in dataset['train']]\n",
    "train_labels = [1 if 'bias_keyword' in text.lower() else 0 for text in train_texts]  # Example labels, replace with real conditions\n",
    "\n",
    "# Tokenize the data and add labels\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Create tokenized dataset\n",
    "# Converting data to a Hugging Face Dataset format and setting format for PyTorch compatibility\n",
    "tokenized_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Split the dataset into training and evaluation\n",
    "train_dataset = tokenized_dataset.select(range(int(0.8 * len(tokenized_dataset))))\n",
    "eval_dataset = tokenized_dataset.select(range(int(0.8 * len(tokenized_dataset)), len(tokenized_dataset)))\n",
    "\n",
    "# Training arguments\n",
    "# Setting up training parameters such as batch size, number of epochs, logging frequency, etc.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save the training results\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    per_device_train_batch_size=8,  # Batch size for training\n",
    "    num_train_epochs=3,  # Number of epochs to train the model\n",
    "    save_steps=500,  # Save the model every 500 steps\n",
    "    logging_dir=\"./logs\",  # Directory to save the logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "# Initializing the Trainer class with the model, training arguments, dataset, and tokenizer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=lambda data: {\n",
    "        'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "        'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "        'labels': torch.tensor([f['label'] for f in data], dtype=torch.long)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# Start training the model using the Trainer instance\n",
    "trainer.train()\n",
    "\n",
    "# Use the fine-tuned model to classify new texts\n",
    "# Function to classify a given text as biased or non-biased using the fine-tuned model\n",
    "def classify_bias(text):\n",
    "    # Encode the input text to get input IDs for the model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Get model predictions\n",
    "    inputs = inputs.to('mps')  # Move inputs to M1 GPU\n",
    "    model.to('mps')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits).item()\n",
    "    \n",
    "    # Check the predicted class\n",
    "    if predicted_class == 1:\n",
    "        return \"Bias detected\"\n",
    "    else:\n",
    "        return \"Non-bias detected\"\n",
    "\n",
    "# Test classification\n",
    "# Testing the classification function with a sample text\n",
    "new_text = \"Some potentially biased text.\"\n",
    "print(classify_bias(new_text))\n",
    "\n",
    "# Note: To run this script, please install the required dependencies by running the following command:\n",
    "# pip install transformers[torch] accelerate>=0.26.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_m1_py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
